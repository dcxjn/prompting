{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dENdfOSxYz7z",
        "outputId": "615d898a-50a9-411a-a60f-8744cc596dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/prompting'...\n",
            "remote: Enumerating objects: 283, done.\u001b[K\n",
            "remote: Counting objects: 100% (283/283), done.\u001b[K\n",
            "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
            "remote: Total 283 (delta 88), reused 241 (delta 56), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (283/283), 2.11 MiB | 6.89 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dcxjn/prompting.git /content/prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WtLdisLkY0ye"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/prompting')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P8aMfDc7Y0_z"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/prompting')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HbaiDChFGmc",
        "outputId": "98e7b394-dbc1-42af-bf48-e4a8d7b972ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KwF9eXQoYtjC"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "import accelerate\n",
        "\n",
        "from src.utils.image_util import load_image, resize_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p-UOjRhuYtjD"
      },
      "outputs": [],
      "source": [
        "def query(inputs: dict) -> dict:\n",
        "\n",
        "    # model_id = \"llava-hf/llava-v1.6-vicuna-13b-hf\"\n",
        "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "    # Configure BitsAndBytesConfig based on GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "    else:\n",
        "        quantization_config = None  # or configure for CPU if necessary\n",
        "\n",
        "    # Initialize pipeline based on GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        pipe = pipeline(\n",
        "            \"image-to-text\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"quantization_config\": quantization_config},\n",
        "            # device=0,  # commented out for Google Colab\n",
        "        )\n",
        "    else:\n",
        "        pipe = pipeline(\n",
        "            \"image-to-text\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"quantization_config\": quantization_config},\n",
        "        )\n",
        "\n",
        "    image = inputs[\"image\"]\n",
        "\n",
        "    prompt1 = f\"\"\"\n",
        "    Observe the given image and its details.\n",
        "    Provide a detailed step-by-step guide on how a human would complete the task of: {inputs[\"task\"]}.\n",
        "    Link each instruction to an observation in the image in this format: Observation - Instruction.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    Imagine you are in control of a robotic arm with the following commands: {inputs[\"bot_commands\"]}\n",
        "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt3 = f\"\"\"\n",
        "    By referencing an observation in the image, ensure each instruction is accurate. Do not make assumptions.\n",
        "    Check that each instruction is logical.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt1 = \"<image>\\n\" + \"USER: \" + prompt1 + \"​\\nASSISTANT: \"\n",
        "\n",
        "    output1 = pipe(\n",
        "        image, prompt=user_prompt1, generate_kwargs={\"max_new_tokens\": 1024}\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== OUTPUT 1 ===\\n\") # for debugging\n",
        "    print(output1[0][\"generated_text\"])\n",
        "\n",
        "    user_prompt2 = (\n",
        "        \"<image>\\n\"\n",
        "        + output1[0][\"generated_text\"]\n",
        "        + \"\\n\\nUSER: \"\n",
        "        + prompt2\n",
        "        + \"​\\nASSISTANT: \"\n",
        "    )\n",
        "\n",
        "    output2 = pipe(\n",
        "        image, prompt=user_prompt2, generate_kwargs={\"max_new_tokens\": 1024}\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== OUTPUT 2 ===\\n\") # for debugging\n",
        "    print(output2[0][\"generated_text\"])\n",
        "\n",
        "    user_prompt3 = (\n",
        "        \"<image>\\n\"\n",
        "        + output2[0][\"generated_text\"]\n",
        "        + \"\\n\\nUSER: \"\n",
        "        + prompt3\n",
        "        + \"​\\nASSISTANT: \"\n",
        "    )\n",
        "\n",
        "    output3 = pipe(\n",
        "        image, prompt=user_prompt3, generate_kwargs={\"max_new_tokens\": 1024}\n",
        "    )\n",
        "\n",
        "    return {\"bot_inst\": output3[0][\"generated_text\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kIXu7ozxYtjE"
      },
      "outputs": [],
      "source": [
        "# Robot commands available\n",
        "bot_commands = \"\"\"\n",
        "    1. move_to(x, y)\n",
        "    2. grab(object)\n",
        "    3. release(object)\n",
        "    4. push(object)\n",
        "    5. pull(object)\n",
        "    6. rotate(angle)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8XDkY34AYtjE"
      },
      "outputs": [],
      "source": [
        "# image_path = input(\"Enter the path of the image: \")\n",
        "# image_path = r\"images/fridge_lefthandle.jpg\"\n",
        "# image_path = r\"images/housedoor_knob_push.jpg\"\n",
        "# image_path = r\"images/browndoor_knob_pull.jpg\"\n",
        "image_path = r\"images/labdoor_straighthandle_pull.jpg\"\n",
        "# image_path = r\"images/bluedoor_knob_push.jpg\"\n",
        "# image_path = r\"images/whitetable.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MSJy7iGDYtjE"
      },
      "outputs": [],
      "source": [
        "resize_image(image_path, image_path)\n",
        "image = load_image({\"image_path\": image_path})[\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu2VlUFuYtjE",
        "outputId": "c1d510f7-350d-4b8b-9dd4-ea80c3c776e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the task to be performed: open the door\n"
          ]
        }
      ],
      "source": [
        "# Define the task to be performed\n",
        "task = input(\"Enter the task to be performed: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "268103cc652447b59f64290dd3146945",
            "4ab34e11e9ad4e1f9c2595ab15af639b",
            "5811e7de58c34612b64364518685d35f",
            "049ad9d5699b46509e1904af3083ad2a",
            "9b24637f997947479d419d6addde1710",
            "54d1055523894d918682a3976f599e84",
            "8d8fbf404f9a4f2bbb3c93b1312a3631",
            "f1c9df4f7c0c44aa9ad180330f3c5b0a",
            "aefec216fd4549f383f62002165bb966",
            "3c776a3c4d1f4c55ac89640839556d94",
            "24af45f5588948abb35fec352f1ea6f9"
          ]
        },
        "id": "y3YPwcFDYtjE",
        "outputId": "70ff91e2-7d83-4f24-8639-7db7f8c246f4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "268103cc652447b59f64290dd3146945",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== OUTPUT 1 ===\n",
            "\n",
            "\n",
            "USER: \n",
            "    Observe the given image and its details.\n",
            "    Provide a detailed step-by-step guide on how a human would complete the task of: open the door.\n",
            "    Link each instruction to an observation in the image in this format: Observation - Instruction.\n",
            "    ​\n",
            "ASSISTANT: 1. The door is closed.\n",
            "2. The handle of the door is located on the right side.\n",
            "3. The door is made of metal.\n",
            "4. The door is gray in color.\n",
            "5. The door is located in a room.\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. To open the door, first, locate the handle on the right side of the door.\n",
            "2. Grip the handle firmly and pull it towards you.\n",
            "3. As you pull, the door should open, revealing the room inside.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The door is closed.\n",
            "2. The handle of the door is located on the right side.\n",
            "3. The door is made of metal.\n",
            "4. The door is gray in color.\n",
            "5. The door is located in a room.\n",
            "\n",
            "=== OUTPUT 2 ===\n",
            "\n",
            "\n",
            "\n",
            "USER: \n",
            "    Observe the given image and its details.\n",
            "    Provide a detailed step-by-step guide on how a human would complete the task of: open the door.\n",
            "    Link each instruction to an observation in the image in this format: Observation - Instruction.\n",
            "    ​\n",
            "ASSISTANT: 1. The door is closed.\n",
            "2. The handle of the door is located on the right side.\n",
            "3. The door is made of metal.\n",
            "4. The door is gray in color.\n",
            "5. The door is located in a room.\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. To open the door, first, locate the handle on the right side of the door.\n",
            "2. Grip the handle firmly and pull it towards you.\n",
            "3. As you pull, the door should open, revealing the room inside.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The door is closed.\n",
            "2. The handle of the door is located on the right side.\n",
            "3. The door is made of metal.\n",
            "4. The door is gray in color.\n",
            "5. The door is located in a room.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "ASSISTANT: 1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The robotic arm is in front of the door.\n",
            "2. The arm is holding the door handle.\n",
            "3. The door is open.\n",
            "4. The arm is rotated to the left.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move\\_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The robotic arm is in front of the door.\n",
            "2. The arm is holding the door handle.\n",
            "3. The door is open.\n",
            "4. The arm is rotated to the left.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move\\_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The robotic arm is in front of the door.\n",
            "2. The arm is holding the door handle.\n",
            "3. The door is open.\n",
            "4. The arm is rotated to the left.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move\\_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The robotic arm is in front of the door.\n",
            "2. The arm is holding the door handle.\n",
            "3. The door is open.\n",
            "4. The arm is rotated to the left.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move\\_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Move the robotic arm to the door.\n",
            "2. Grab the door handle.\n",
            "3. Release the door handle.\n",
            "4. Push the door open.\n",
            "5. Rotate the arm to the left.\n",
            "\n",
            "Observations:\n",
            "\n",
            "1. The robotic arm is in front of the door.\n",
            "2. The arm is holding the door handle.\n",
            "3. The door is open.\n",
            "4. The arm is rotated to the left.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move\\_to(x,\n"
          ]
        }
      ],
      "source": [
        "result = query(\n",
        "    {\n",
        "        \"image\": image,\n",
        "        \"task\": task,\n",
        "        \"bot_commands\": bot_commands,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVSX9L20YtjF",
        "outputId": "351712ca-dd73-4be7-ddaa-1ac4dc6e24db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "USER: \n",
            "    Observe the given image and its details.\n",
            "    Provide a detailed step-by-step guide on how a human would complete the task of: open the door.\n",
            "    Link each instruction to an observation in the image in this format: Observation - Instruction.\n",
            "    ​\n",
            "ASSISTANT: 1. Observe the given image and its details.\n",
            "2. Identify the blue door with a knob and a potted plant next to it.\n",
            "3. Approach the door and prepare to open it.\n",
            "4. Observe the knob on the door.\n",
            "5. Gently turn the knob to the left, which will open the door.\n",
            "6. As the door opens, observe the interior of the house.\n",
            "\n",
            "The image shows a blue door with a knob and a potted plant next to it. The door is open, revealing the interior of the house.\n",
            "\n",
            "USER: \n",
            "    Imagine you are in control of a robotic arm with the following commands: \n",
            "    1. move_to(x, y)\n",
            "    2. grab(object)\n",
            "    3. release(object)\n",
            "    4. push(object)\n",
            "    5. pull(object)\n",
            "    6. rotate(angle)\n",
            "\n",
            "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
            "    ​\n",
            "ASSISTANT: 1. move\\_to(the blue door)\n",
            "2. grab(the knob)\n",
            "3. release(the knob)\n",
            "4. push(the door)\n",
            "5. pull(the door)\n",
            "6. rotate(90 degrees)\n",
            "\n",
            "The robot would first move to the blue door, then grab the knob using its robotic arm. After releasing the knob, the robot would push the door open and pull it to a 90-degree angle, allowing it to be fully open.\n",
            "\n",
            "USER: \n",
            "    By referencing an observation in the image, ensure each instruction is accurate. Do not make assumptions.\n",
            "    Check that each instruction is logical.\n",
            "    ​\n",
            "ASSISTANT: 1. move\\_to(the blue door)\n",
            "2. grab(the knob)\n",
            "3. release(the knob)\n",
            "4. push(the door)\n",
            "5. pull(the door)\n",
            "6. rotate(90 degrees)\n",
            "\n",
            "The instructions provided are accurate and logical. The robot would first move to the blue door, then grab the knob using its robotic arm. After releasing the knob, the robot would push the door open and pull it to a 90-degree angle, allowing it to be fully open.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n==========\\n\")\n",
        "print(result[\"bot_inst\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "268103cc652447b59f64290dd3146945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ab34e11e9ad4e1f9c2595ab15af639b",
              "IPY_MODEL_5811e7de58c34612b64364518685d35f",
              "IPY_MODEL_049ad9d5699b46509e1904af3083ad2a"
            ],
            "layout": "IPY_MODEL_9b24637f997947479d419d6addde1710"
          }
        },
        "4ab34e11e9ad4e1f9c2595ab15af639b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d1055523894d918682a3976f599e84",
            "placeholder": "​",
            "style": "IPY_MODEL_8d8fbf404f9a4f2bbb3c93b1312a3631",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5811e7de58c34612b64364518685d35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c9df4f7c0c44aa9ad180330f3c5b0a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aefec216fd4549f383f62002165bb966",
            "value": 3
          }
        },
        "049ad9d5699b46509e1904af3083ad2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c776a3c4d1f4c55ac89640839556d94",
            "placeholder": "​",
            "style": "IPY_MODEL_24af45f5588948abb35fec352f1ea6f9",
            "value": " 3/3 [01:07&lt;00:00, 22.41s/it]"
          }
        },
        "9b24637f997947479d419d6addde1710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54d1055523894d918682a3976f599e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8fbf404f9a4f2bbb3c93b1312a3631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1c9df4f7c0c44aa9ad180330f3c5b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aefec216fd4549f383f62002165bb966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c776a3c4d1f4c55ac89640839556d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24af45f5588948abb35fec352f1ea6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}