{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dcxjn/prompting.git /content/prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/prompting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/prompting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install einops \n",
    "!pip install transformers_stream_generator\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "from src.utils.image_util import load_image, resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(inputs: dict) -> dict:\n",
    "\n",
    "    # Set tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "\n",
    "    # Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n",
    "    # model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "    prompt1 = f\"\"\"\n",
    "    Observe the given image and its details.\n",
    "    Provide a detailed step-by-step guide on how a human would complete the task of: {inputs[\"task\"]}.\n",
    "    Link each instruction to an observation in the image in this format: Observation - Instruction.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt2 = f\"\"\"\n",
    "    Imagine you are in control of a robotic arm with the following commands: {inputs[\"bot_commands\"]}\n",
    "    Given the human instructions you have generated, provide a guide on how the robot would complete the task.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt3 = f\"\"\"\n",
    "    By referencing an observation in the image, ensure each instruction is accurate. Do not make assumptions.\n",
    "    Check that each instruction is logical.\n",
    "    \"\"\"\n",
    "\n",
    "    query = tokenizer.from_list_format([\n",
    "        {'image': {inputs['image']}},\n",
    "        {'text': prompt1},\n",
    "    ])\n",
    "    output1, history = model.chat(tokenizer, query=query, history=None)\n",
    "    print(\"\\n=== OUTPUT 1 ===\\n\") # for debugging\n",
    "    print(output1)\n",
    "\n",
    "    output2, history = model.chat(tokenizer, prompt2, history=history)\n",
    "    print(\"\\n=== OUTPUT 2 ===\\n\") # for debugging\n",
    "    print(output2)\n",
    "\n",
    "    output3, history = model.chat(tokenizer, prompt3, history=history)\n",
    "    \n",
    "    return {\"bot_inst\": output3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robot commands available\n",
    "bot_commands = \"\"\"\n",
    "    1. move_to(x, y)\n",
    "    2. grab(object)\n",
    "    3. release(object)\n",
    "    4. push(object)\n",
    "    5. pull(object)\n",
    "    6. rotate(angle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = input(\"Enter the path of the image: \")\n",
    "# image_path = r\"images\\fridge_lefthandle.jpg\"\n",
    "# image_path = r\"images\\housedoor_knob_push.jpg\"\n",
    "# image_path = r\"images\\browndoor_knob_pull.jpg\"\n",
    "# image_path = r\"images\\labdoor_straighthandle_pull.jpg\"\n",
    "image_path = r\"images/bluedoor_knob_push.jpg\"\n",
    "# image_path = r\"images\\whitetable.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_image(image_path, image_path)\n",
    "image = load_image({\"image_path\": image_path})[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task to be performed\n",
    "task = input(\"Enter the task to be performed: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query(\n",
    "    {\n",
    "        \"image\": image,\n",
    "        \"task\": task,\n",
    "        \"bot_commands\": bot_commands,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==========\\n\")\n",
    "print(result[\"bot_inst\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
